{
  "id": "EUVD-2023-0118",
  "enisaUuid": "96ac99e7-5180-334b-96ff-ee645af1fe41",
  "description": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.",
  "datePublished": "Apr 5, 2023, 12:00:00 AM",
  "dateUpdated": "Feb 12, 2025, 4:24:39 PM",
  "baseScore": 9.8,
  "baseScoreVersion": "3.1",
  "baseScoreVector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H",
  "references": [
    "https://twitter.com/rharang/status/1641899743608463365/photo/1",
    "https://github.com/hwchase17/langchain/pull/1119",
    "https://github.com/hwchase17/langchain/issues/814",
    "https://github.com/hwchase17/langchain/issues/1026",
    "https://nvd.nist.gov/vuln/detail/CVE-2023-29374",
    "https://github.com/langchain-ai/langchain",
    "https://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2023-18.yaml"
  ],
  "aliases": [
    "CVE-2023-29374",
    "PYSEC-2023-18"
  ],
  "assigner": "mitre",
  "epss": 2.88,
  "enisaIdProduct": [
    {
      "id": "701e35d0-631c-33ba-bfc1-6a38b725f420",
      "product": {
        "name": "n/a"
      },
      "product_version": "n/a"
    }
  ],
  "enisaIdVendor": [
    {
      "id": "1a3a3357-f198-3408-8357-ff8bf0466837",
      "vendor": {
        "name": "n/a"
      }
    }
  ]
}